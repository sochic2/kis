### 단일 퍼셉트론 
- 계단함수를 이용해 1아니면 0이라는 신호만 보냄

### 신경망
- 시그모이드 함수 이용
- 연속적인 실수가 흐름

### 비선형 함수
- 선형이 아닌 함수
- 선형함수 : f(x) = ax+b
- 층을 쌓는 혜택을 얻고 싶다면 비선형 함수를 사용해야함 ex) XOR

### ReLU(Rectified Linear Unit) : 입력이 0을 넘으면 그 입력 그대로 출력, 0이하면 0을 출력하는 함수
- 어떤 면에선 시그모이드 함수와 비슷
- 연속적인 실수를 반환해준다는 점
- ReLU는 미분값이 동일. 1로



### 행열곱?

- `2`X3 행열과 3X`2`행열 하면 결과값 2X2 행열 나옴

- 행렬의 내적을 구할 때 (1)
  - 행렬 A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 같아야 함.
  - 위의 예시는 둘 모두 원소가 3개라 같음.
  - 2 X 3 행렬 A와 2 X 2 행렬 C를 곱하면 다음처럼 오류 발생.

- 행렬의 내적을 구할 때 (2)
  - 두 행렬의 대응하는 차원의 원소 수를 일치시켜야 함
  - A와 B의 대응하는 차원의 원소 수가 같아야 함.
  - 계산 결과인 행렬 C의 형상은 행렬 A의 행수와 행렬 B의 열수가 됨.
- 행렬의 내적을 구할 때 (3)
  - A가 2차원 행렬이고 B가 1차원 배열일 때도 '대응하는 차원의 원소 수를 일치시켜라'는 원칙이 똑같이 적용됨.

- cpu : 고차원적인 연산을 잘함

- gpu : 단순한 곱셈연산과 같은 것을 잘함. 따라서 신경망 구현할 때 쓰는 행열곱 같은 것에 최적화되어있다.
- 50년대에 이미 멀티레이어를 통해(퍼셉트론) 많은 문제를 해결할 수 있는 것을 발견했지만 gpu의 발전으로 새로운 기술을 접할 수 있는 세상이 됨.



### 손글씨 숫자 인식

- 입력층 뉴런 784(28*28 픽셀을 쭉 편다고 생각), 출력층 뉴런 10개로 구성(0~9 숫자 10개니깐)
- 은닉층은 총 2개, 은닉층 뉴런의 개수는 임의로 결정
- 한마디로
  - 1 X 784,  O X O , O X O, O X 10



### 소프트맥스

- 유의사항 오버플로(Overflow) 가 날수 있다. float64, int32같은것 쓰기 때문에 2**32까지만 표현 가능하기때문에

- 소프트맥스 함수 개선 사항

  - 첫 번째 번형에서 C라는 임의의 정수를 분자와 분보 양쪽에 곱함
  - C를 지수 함수 exp() 안으로 옮겨 logC로 만듬
  - logC를 C'라는 새로운 기호로 바꿈
  - 소프트맥스의 지수 함수를 계산할 어떤 정수를 더해도 (혹은 빼도) 결과는 바뀌지 않음
  - **오버플로를 막을 목적으로는 입력 신호 중 최대값을 이용하는 것이 일반적**

- ```python
  #오버플로 예시
  a = np.array([1010, 1000, 990])
  np.exp(a) / np.sum(np.exp(a)) # 소프트맥스 함수의 계산
  # array([ nan,  nan,  nan])   # 제대로 계산되지 않는다.
  
  def softmax(a):
      c = np.max(a)
      exp_a = np.exp(a-c) # 오버플로 대책
      sum_exp_a = np.sum(exp_a)
      y = exp_a / sum_exp_a
      
      return y
  ```



- **소프트맥스 함수 특징**

  ```python
  a = np.array([0.3, 2.9, 4.0])
  y = softmax(a)
  print(y)
  np.sum(y)
  ```

  

  - **소프트맥스 함수 출력의 총합은 1. 출력을 '확률'로 해석할 수 있음.**
  - y[0]의 확률 0.018(1.8%), y[1]의 확률 0.245(24.5%), y[2]의 확률 0.737(73.7%)로 해석가능
  - "2번째 원소의 확률이 가장 높으니, 답은 2번 클래스다"
  - 소프트맥스 함수를 이용함으로써 문제를 확률적(통계적)으로 대응할 수 있게 됨
  - **소프트맥스를 적용해도 각 원소의 대소 관계는 변하지 않음**
    - 이유: 단조 증가함수(a<=b일 때 f(a)<=f(b)가 성립하는 함수)이기 때문
    - 추론 단계에서는 출력층의 소프트맥스 함수를 생략하는 것이 일반적
  - 학습 시킬 때에는 출력층에서 소프트맥스 함수를 사용



### svm, neuralnetwork

25% => 16% 오차율 급감(AlexNet 등장)

end to end를 이용해 스스로 feature 특징을 찾아내도록 함

이전에는 인간은 어떤 특성을갖고있다. 이런식으로 진행햇는데...

그래서 그 이후 정확도 극도로 발달하기 시작



vector => matrix => tensor



